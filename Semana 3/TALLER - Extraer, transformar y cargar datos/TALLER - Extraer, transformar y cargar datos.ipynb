{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b19a51c305a7880951a3493b81997fa3",
     "grade": false,
     "grade_id": "header-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Extracción, transformación y carga de datos: `pyspark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "545f3069f622434931d0aed232433155",
     "grade": false,
     "grade_id": "header-desc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En este taller extraerás, transformarás y cargarás tablas, haciendo uso de `pyspark` para interactuar con bases de datos relacionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8114ade8404b883dd1c9aec05ee86b2b",
     "grade": false,
     "grade_id": "header-habi-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Habilidades en práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec1cca48c4858868803cc077c101d691",
     "grade": false,
     "grade_id": "header-habi",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Al realizar este taller podrás revisar tu progreso para:\n",
    "\n",
    "**1.** Extraer y transformar tablas de bases de datos relacionales con operaciones de algebra relacional en `pyspark`. <br>\n",
    "**2.** Crear y cargar tablas en bases de datos relacionales con SQL (_Structured Query Language_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db386060214c4d5b050c05946b6434c0",
     "grade": false,
     "grade_id": "header-instruc-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbd8a4456a32019e7efba2fbdfe165cb",
     "grade": false,
     "grade_id": "header-instruc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En cada uno de los siguientes ejercicios deberás escribir el código solicitado estrictamente en las celdas indicadas para ello, teniendo en cuenta las siguientes recomendaciones:\n",
    "\n",
    "* No crear, eliminar o modificar celdas de este Notebook (salvo lo que se te indique), pues puede verse afectado el proceso de calificación automática.\n",
    "\n",
    "* La calificación se realiza de manera automática con datos diferentes a los proporcionados en este taller. Por consiguiente, tu código debe funcionar para diferentes instancias de cada uno de los ejercicios; una instancia hace referencia a los posibles valores de los parámetros.\n",
    "\n",
    "* La calificación de cada ejercicio depende del valor que retorne la función especificada en su enunciado. Por lo tanto, aunque implementes funciones adicionales, es escencial que utilices los nombres propuestos en los enunciados de los ejercicios para implementar la función definitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68d388c33cbffb74323dd75af9db7dea",
     "grade": false,
     "grade_id": "bloque-ejer",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Ejercicios\n",
    "En la siguente celda encuentras declarados los paquetes necesarios para el desarollo de este taller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c57fda61db75e34592731234ca3840",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Esta celda no es modificable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d67d627edb28de23f37721adae2e72a",
     "grade": false,
     "grade_id": "bloque-init",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En la siguiente celda encuentras la inicialización de una sesión de `pyspark`. Puedes editar la configuración según tu criterio, pero recomendamos mantener los valores predefinidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Edwin:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Instancia_Taller_PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2224ebb1a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta celda SÍ es modificable\n",
    "#debemos descargar los dos programas de la siguiente pagina:\n",
    "#https://www.oracle.com/java/technologies/downloads/\n",
    "#https://spark.apache.org/downloads.html\n",
    "#https://github.com/steveloughran/winutils\n",
    "#https://www.youtube.com/watch?v=wt2wM8C2SXA            super util\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"C:\\\\hadoop\\\\bin\\\\winutils.exe\", \"chmod\", \"777\", \"C:\\\\tmp\\\\hive\"])\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .appName(\"Instancia_Taller_PySpark\") \\\n",
    "                    .config(\"spark.sql.warehouse.dir\", \"./Archivos/\") \\\n",
    "                    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:3.5.1_0.20.4\") \\\n",
    "                    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e458ce21ed6dabaa4a99cc5f3a0e173",
     "grade": false,
     "grade_id": "bloque-start-over",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Si en algún momento deseas restablecer el estado de Spark y borrar las bases de datos, puedes correr el siguiente código. Asegurate de reemplazar la ruta al directorio de tu base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda SÍ es modificable\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Esta instrucción permite borrar bases de datos persistentes.\n",
    "# Si cambiaste el atributo `spark.sql.warehouse.dir` en la configuración\n",
    "# de la sesión, debes reflejar el cambio en esta instrucción.\n",
    "\n",
    "# shutil.rmtree('./Archivos/<nombre_db>.db/')\n",
    "\n",
    "# Esta instrucción borra la base de datos temporal de PySpark.\n",
    "# shutil.rmtree('./metastore_db/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bee373ad53723a4e570ef8c14e1d3370",
     "grade": false,
     "grade_id": "enun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Harmonialpes, una empresa dedicada a la distribución al por mayor de harmónicas, ha tenido ventas favorables a lo largo del año. Con el gran volumen de harmónicas, el equipo de ventas se ha dado cuenta de que registrarlas en una hoja de Excel compartida no es una forma viable de hacerle seguimiento al desempeño del negocio y ha decidido migrar a un sistema de bases de datos relacionales. Tú, como experto en analítica y gobierno de datos, has sido encargado con la tarea de esta ambiciosa transformación tecnológica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a16f024f3cc9d68c3ca305c5004d46ef",
     "grade": false,
     "grade_id": "ej1-enun-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72ac04a4f9197d227ec6838a987963ef",
     "grade": false,
     "grade_id": "ej1-enun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En la siguiente celda encuentras declarada la ruta relativa al archivo de Excel que almacena los datos de todas las órdenes de Harmonialpes, cada una con el respectivo cliente que la realiza y el agente de ventas encargado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d9e333329947a24e35e1a4c4512e478",
     "grade": false,
     "grade_id": "ej1-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# No modifiques esta celda\n",
    "\n",
    "ruta = r\"Archivos/tabla_ventas.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74cb61eaf7d95556f8abc4f05a34b5f4",
     "grade": false,
     "grade_id": "ej1-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implementa una función llamada `xlsx_a_dataframe` que reciba por parámetro una cadena de texto como la declarada en la celda anterior y que retorne un `DataFrame` de `pyspark`, resultado de leer el archivo. Debes revisar meticulósamente qué campos deben ser de qué tipo, ya que vas a encontrar valores numéricos, fechas y cadenas de texto.\n",
    "\n",
    "Puedes utilizar métodos de `pandas` para leer los datos, pero no para otros ejercicios del taller.\n",
    "\n",
    "La función debe retornar un `DataFrame` de `pyspark`.\n",
    "\n",
    "Ejecuta tu función con la ruta definida como argumento y guarda el resultado en una variable global llamada `spark_ventas_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74db8000d9e6991af846fc6d76f31f6c",
     "grade": false,
     "grade_id": "ej1_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def xlsx_a_dataframe(ruta):\n",
    "    return spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sheetName\", \"Ventas\") \\\n",
    "    .load(ruta)\n",
    "\n",
    "spark_ventas_df = xlsx_a_dataframe(ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35b6c5fcff66986c862b7249133d7f23",
     "grade": true,
     "grade_id": "ej1_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Felicidades, realizaste este ejercicio correctamente.\n"
     ]
    }
   ],
   "source": [
    "## AUTO-CALIFICADOR\n",
    "\n",
    "# Base variables\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Caso 1: no existe la función.\n",
    "    try:\n",
    "        xlsx_a_dataframe\n",
    "        assert callable(xlsx_a_dataframe)\n",
    "    except:\n",
    "        raise NotImplementedError(\"No existe una función llamada xlsx_a_dataframe.\",)\n",
    "\n",
    "    # Caso 2: la función es interrumpida por errores durante su ejecución.\n",
    "    try:\n",
    "        resultado = xlsx_a_dataframe(ruta)\n",
    "    except:\n",
    "        raise RuntimeError(\"Tu función produce un error al ejecutarse.\")    \n",
    "\n",
    "    # Caso 3: no retorna un DataFrame.\n",
    "    assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
    "\n",
    "    # Caso 4: retorna un dataframe con cantidad de columnas errada\n",
    "    assert len(resultado.columns) == 23, \"Tu función retorna un DataFrame con cantidad de columnas errada.\"\n",
    "\n",
    "    # Caso 5: devuelve un dataframe con cantidad de filas errada\n",
    "    assert resultado.count() == 35, \"Tu función retorna un DataFrame con cantidad de filas errada.\"\n",
    "\n",
    "    # Caso 6: retorna valores no acertados\n",
    "    expected_first_row = [200131, 900, 150, datetime.datetime(2008, 8, 26, 0, 0), 'SOD', 'C00012', 'Steven', 'San Jose', 'San Jose', 'USA', 1, 5000, 7000, 9000, 3000, 'KRFYGJK', 'A012', 'A012', 'Lucida', 'San Jose', 0.12, '044-52981425', 'United States']\n",
    "    expected_last_row = [200124, 500, 100, datetime.datetime(2008, 6, 20, 0, 0), 'SOD', 'C00017', 'Srinivas', 'Bangalore', 'Bangalore', 'India', 2, 8000, 4000, 3000, 9000, 'AAAAAAB', 'A007', 'A007', 'Ramasundar', 'Bangalore', 0.15, '077-25814763', 'India']\n",
    "\n",
    "    assert list(resultado.head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame con valores distintos a los esperados.\"\n",
    "    assert list(resultado.tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame con valores distintos a los esperados.\"\n",
    "\n",
    "    # Caso 7: no guarda el resultado en la variable indicada\n",
    "    try:\n",
    "        spark_ventas_df\n",
    "        assert isinstance(spark_ventas_df, pyspark.sql.dataframe.DataFrame)\n",
    "    except:\n",
    "        raise NotImplementedError(\"No existe un DataFrame llamado spark_ventas_df.\",)\n",
    "\n",
    "except:\n",
    "    # Restaurar variable\n",
    "    ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    # Restaurar variable\n",
    "    ruta = r\"Archivos/tabla_ventas.xlsx\"\n",
    "\n",
    "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffbdbd172d21f55d17e3780bb26a2843",
     "grade": false,
     "grade_id": "ej2-enun-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "981713ab79c2fd4216e57d65da4b0347",
     "grade": false,
     "grade_id": "ej2-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "La tabla de ventas, en su estado actual, incluye información del cliente y el agente de ventas para cada orden y, por lo tanto, tiene una dimensión que es incómoda de manejar para los usuarios de los datos. También, cada cliente y cada agente está registrado en una o más ventas, lo que quiere decir que hay mucha información redundante en la tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ce8d4156dbd1a4d0bb74097eb6b1638",
     "grade": false,
     "grade_id": "ej2-task-cont1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Haciendo uso de la variable que definiste, `spark_ventas_df`, crea una función llamada `desagregar_df` que reciba por parámetro un `DataFrame` con los mismos campos de la variable `spark_ventas_df` y retorne una tupla con tres nuevos `DataFrame` de `pyspark`. Los `DataFrame` deben contener las combinaciones únicas existentes de las columnas de cada categoría (orden, cliente, agente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6abeb25969ba54b7d38cc52dccb51a6",
     "grade": false,
     "grade_id": "ej2-task-cont2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Almacena cada `DataFrame` del resultado de la función `desagregar_df` en las siguientes variables globales, según su descripción:\n",
    "\n",
    "- `spark_ordenes_df`: las columnas que describen las órdenes de compra. Estas columnas tienen nombres de la forma `\"Order_<campo>\"`. <br><br>\n",
    "    - La columna `\"Order_Number\"` define individualmente cada orden de compra.<br><br>\n",
    "    - Para poder no perder información acerca de qué cliente realizó cada orden, debemos incluir en este `DataFrame` la columna `Customer_Code`.<br><br>\n",
    "    - Para poder no perder información acerca de qué agente estuvo encargado de cada orden, debemos incluir en este `DataFrame` la columna `Agent_Code`.<br><br>\n",
    "    - Este `DataFrame` no puede contener información adicional de los clientes ni de los agentes.<br><br>\n",
    "\n",
    "- `spark_clientes_df`: las columnas que describen a los clientes. Estas columnas tienen nombres de la forma `\"Customer_<campo>\"`. <br><br>\n",
    "    - La columna `\"Customer_Code\"` define individualmente a cada cliente.<br><br>\n",
    "    - De cada cliente se encarga un único agente. Con el fin de respetar la relación de negocios entre los agentes y sus respectivos clientes, debemos incluir en este `DataFrame` la columna `Agent_Code`.<br><br>\n",
    "    - Este `DataFrame` no puede contener información adicional de las ordenes ni de los agentes.<br><br>\n",
    "    \n",
    "- `spark_agentes_df`: las columnas que describen a los agentes. Estas columnas tienen nombres de la forma `\"Agent_<campo>\"`. <br><br>\n",
    "    - La columna `\"Agent_Code\"` define individualmente a cada agente.<br><br>\n",
    "    - Este `DataFrame` no puede contener información adicional de las ordenes ni de los clientes.\n",
    "    \n",
    "Asegúrate de ordenar las tablas por la columna que define los registros individualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a196bd9e0c9301e8cc99e0a1fe1e7d1b",
     "grade": false,
     "grade_id": "ej2-task-ayuda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ayuda: puedes utilizar el método `DataFrame.colRegex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a340755d579b5734896ce85bc57b92b1",
     "grade": false,
     "grade_id": "ej2_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def desagregar_df(spark_df):\n",
    "    order_df = spark_df.select([column_name for column_name in spark_df.columns if 'Order' in column_name] \n",
    "                               + [col('Agent_Code16').alias('Agent_Code')] \n",
    "                               + [col('Customer_Code')] ) \\\n",
    "                               .dropDuplicates() \\\n",
    "                               .sort(col('Order_Number').asc())\n",
    "    \n",
    "    customer_df = spark_df.select([column_name for column_name in spark_df.columns if 'Customer' in column_name] \n",
    "                                  + [col('Agent_Code16').alias('Agent_Code')]) \\\n",
    "                                  .dropDuplicates() \\\n",
    "                                  .sort(col('Customer_Code').asc())\n",
    "    \n",
    "    agent_df = spark_df.select([col('Agent_Code16').alias('Agent_Code')] \n",
    "                               + [column_name for column_name in spark_df.columns if 'Agent' in column_name and 'Code' not in column_name]) \\\n",
    "                               .dropDuplicates() \\\n",
    "                               .sort(col('Agent_Code').asc())\n",
    "\n",
    "\n",
    "    return (order_df, customer_df, agent_df)\n",
    "\n",
    "spark_ordenes_df = desagregar_df(spark_ventas_df)[0]\n",
    "spark_clientes_df = desagregar_df(spark_ventas_df)[1]\n",
    "spark_agentes_df = desagregar_df(spark_ventas_df)[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2df185acc5899a0e69bbb64e8c44403a",
     "grade": true,
     "grade_id": "ej2_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Felicidades, realizaste este ejercicio correctamente.\n"
     ]
    }
   ],
   "source": [
    "## AUTO-CALIFICADOR\n",
    "\n",
    "# Base variables\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "# Caso 1: no existe la función.\n",
    "try:\n",
    "    desagregar_df\n",
    "    assert callable(desagregar_df)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe una función llamada desagregar_df.\",)\n",
    "\n",
    "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
    "try:\n",
    "    resultado = desagregar_df(spark_ventas_df)\n",
    "except:\n",
    "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")    \n",
    "\n",
    "# Caso 3: no retorna una tupla.\n",
    "assert isinstance(resultado, tuple), f\"Tu función debe retornar un objeto de tipo '{tuple.__name__}'.\"\n",
    "\n",
    "# Caso 4: no retorna objetos tipo DataFrame en la tupla.\n",
    "for i in range(3):\n",
    "    assert isinstance(resultado[i], pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar una {tuple.__name__} con elementos de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
    "\n",
    "# Caso 5: retorna un dataframe con cantidad de columnas errada\n",
    "assert len(resultado[0].columns) == 7, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "assert len(resultado[1].columns) == 12, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "assert len(resultado[2].columns) == 6, \"Tu función retorna un DataFrame en la tupla con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "\n",
    "# Caso 6: devuelve un dataframe con cantidad de filas errada\n",
    "assert resultado[0].count() == 34, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
    "assert resultado[1].count() == 25, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
    "assert resultado[2].count() == 12, \"Tu función retorna un DataFrame en la tupla con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
    "\n",
    "# Caso 7: los dataframe no contienen las columnas debidas\n",
    "assert \"Agent_Code\" in resultado[0].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "assert \"Customer_Code\" in resultado[0].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "assert sum(i[:len(\"Order_\")] == \"Order_\" for i in resultado[0].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "\n",
    "assert \"Agent_Code\" in resultado[1].columns, \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "assert sum(i[:len(\"Customer_\")] == \"Customer_\" for i in resultado[1].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "\n",
    "assert sum(i[:len(\"Agent_\")] == \"Agent_\" for i in resultado[2].columns), \"Tu función retorna un DataFrame en la tupla sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "\n",
    "# Caso 8: retorna valores no acertados\n",
    "expected_first_row = [200100, 1000, 600, datetime.datetime(2008, 8, 1, 0, 0), 'SOD', 'A003', 'C00013']\n",
    "expected_last_row = [200135, 2000, 800, datetime.datetime(2008, 9, 16, 0, 0), 'SOD', 'A010', 'C00007']\n",
    "assert list(resultado[0].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "assert list(resultado[0].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "\n",
    "expected_first_row = ['C00001', 'Micheal', 'New York', 'New York', 'USA', 2, 3000, 5000, 2000, 6000, 'CCCCCCC', 'A008']\n",
    "expected_last_row = ['C00025', 'Ravindran', 'Bangalore', 'Bangalore', 'India', 2, 5000, 7000, 4000, 8000, 'AVAVAVA', 'A011']\n",
    "assert list(resultado[1].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "assert list(resultado[1].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "\n",
    "expected_first_row = ['A001', 'Subbarao', 'Bangalore', 0.14, '077-12346674', 'India']\n",
    "expected_last_row = ['A012', 'Lucida', 'San Jose', 0.12, '044-52981425', 'United States']\n",
    "assert list(resultado[2].head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "assert list(resultado[2].tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame en la tupla con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "\n",
    "# Caso 9: no existen las variables indicadas\n",
    "try:\n",
    "    spark_ordenes_df\n",
    "    assert isinstance(spark_ordenes_df, pyspark.sql.dataframe.DataFrame)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe un DataFrame llamado spark_ordenes_df.\",)\n",
    "    \n",
    "try:\n",
    "    spark_clientes_df\n",
    "    assert isinstance(spark_clientes_df, pyspark.sql.dataframe.DataFrame)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe un DataFrame llamado spark_clientes_df.\",)\n",
    "    \n",
    "try:\n",
    "    spark_agentes_df\n",
    "    assert isinstance(spark_agentes_df, pyspark.sql.dataframe.DataFrame)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe un DataFrame llamado spark_agentes_df.\",)\n",
    "\n",
    "# Caso 10: no guarda el resultado en las variables acertadas\n",
    "assert spark_ordenes_df.collect() == resultado[0].collect(), \"La variable spark_ordenes_df no guarda el primer DataFrame de la tupla.\"\n",
    "assert spark_clientes_df.collect() == resultado[1].collect(), \"La variable spark_clientes_df no guarda el segundo DataFrame de la tupla.\"\n",
    "assert spark_agentes_df.collect() == resultado[2].collect(), \"La variable spark_agentes_df no guarda el tercer DataFrame de la tupla.\"\n",
    "\n",
    "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4412615a4912cb1aa448d225705f3993",
     "grade": false,
     "grade_id": "ej3-enun-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1deb7ba8ede390c268ed982c519b0310",
     "grade": false,
     "grade_id": "ej3-enun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En preparación para la transformación tecnológica, Harmonialpes ha estado recopilando más y más información para almacenar en bases de datos y aprovechar al máximo la inversión en infraestructura y tecnología. En aras de poder acomodar nuevos esquemas de datos y gran diversidad de información, han pedido una función que pueda crear nuevas bases de datos, en las cuales puedan almacenar permanentemente nuevas tablas y sus relaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "534db846b7c912b724f38ccf3c14ff53",
     "grade": false,
     "grade_id": "ej3-enun-cont",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "En la siguiente celda encuentras declarada una lista de cadenas de texto, cada una con el nombre de una base de datos que necesitan para almacenar su información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3be1c574a8bd8eb43e89ac6f73cdc4f",
     "grade": false,
     "grade_id": "ej3-data",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# No modifiques esta celda\n",
    "\n",
    "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd86780df60c8ca6e3aaf2f56af47323",
     "grade": false,
     "grade_id": "ej3-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implementa una función llamada `crear_varios_db` que reciba por parámetro una lista de cadenas de texto como la declarada en la celda anterior y que retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener una única columna llamada `\"namespace\"` cuyos valores sean las bases de datos en el directorio de archivos (no debe incluir `default`). La función no debe crear las bases de datos que ya existan en el directorio. \n",
    "\n",
    "La función debe retornar un `DataFrame` de `pyspark`.\n",
    "\n",
    "Ejecuta tu función con la lista de nombres a crear definida como argumento y guarda el resultado en una variable global llamada `spark_bases_de_datos_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff408280ec24f14ab5c0cd9d80b01ec7",
     "grade": false,
     "grade_id": "ej3_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o323.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 91.0 failed 1 times, most recent failure: Lost task 0.0 in stage 91.0 (TID 123) (Edwin executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rows)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mcrear_varios_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnombres_a_crear\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mcrear_varios_db\u001b[1;34m(nombres)\u001b[0m\n\u001b[0;32m     10\u001b[0m rows \u001b[38;5;241m=\u001b[39m [Row(namespace\u001b[38;5;241m=\u001b[39mpath) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m databases]\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rows)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o323.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 91.0 failed 1 times, most recent failure: Lost task 0.0 in stage 91.0 (TID 123) (Edwin executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import glob\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def crear_varios_db(nombres):\n",
    "    for nombre in nombres: \n",
    "        spark.sql(f'CREATE DATABASE IF NOT EXISTS {nombre};')\n",
    "    databases = glob.glob(\"Archivos/*.db\")\n",
    "\n",
    "    rows = [Row(namespace=path) for path in databases]\n",
    "\n",
    "    df = spark.createDataFrame(rows)\n",
    "    return df.show()\n",
    "\n",
    "crear_varios_db(nombres_a_crear)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae9d683b8da1c4ec8fb65293271b6f4e",
     "grade": true,
     "grade_id": "ej3_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## AUTO-CALIFICADOR\n",
    "\n",
    "# Base variables\n",
    "import pyspark\n",
    "\n",
    "\n",
    "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
    "\n",
    "try:    \n",
    "    # Caso 1: no existe la función.\n",
    "    try:\n",
    "        crear_varios_db\n",
    "        assert callable(crear_varios_db)\n",
    "    except:\n",
    "        raise NotImplementedError(\"No existe una función llamada crear_varios_db.\",)\n",
    "\n",
    "    # Caso 2: la función es interrumpida por errores durante su ejecución.\n",
    "    try:\n",
    "        resultado = crear_varios_db(nombres_a_crear)\n",
    "        nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
    "    except:\n",
    "        raise RuntimeError(\"Tu función produce un error al ejecutarse.\")    \n",
    "\n",
    "    # Caso 3: no retorna un DataFrame.\n",
    "    assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
    "\n",
    "    # Caso 4: retorna un dataframe con cantidad de columnas errada\n",
    "    assert len(resultado.columns) == 1, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "\n",
    "    # Caso 5: devuelve un dataframe con cantidad de filas errada\n",
    "    assert resultado.count() >= len(nombres_a_crear), \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
    "    \n",
    "    # Caso 6: los dataframe no contienen las columnas debidas\n",
    "    assert \"namespace\" in resultado.columns, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "    \n",
    "    # Caso 7: retorna valores no acertados\n",
    "    assert resultado.withColumn(\"was_requested\", functions.col(\"namespace\") \\\n",
    "                                         .isin(nombres_a_crear) \\\n",
    "                                         .cast(\"long\")) \\\n",
    "                                         .agg(functions.sum(\"was_requested\")).collect()[0][0] == len(nombres_a_crear), \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegúrate de no tener filas repetidas o faltantes.\"\n",
    "\n",
    "    # Caso 8: no existen las variables indicadas\n",
    "    try:\n",
    "        spark_bases_de_datos_df\n",
    "        assert isinstance(spark_bases_de_datos_df, pyspark.sql.dataframe.DataFrame)\n",
    "    except:\n",
    "        raise NotImplementedError(\"No existe un DataFrame llamado spark_bases_de_datos_df.\",)\n",
    "        \n",
    "    # Caso 9: no crea las bases de datos\n",
    "    data = [[i] for i in nombres_a_crear]\n",
    "    assert set(spark.sql(\"SHOW DATABASES;\").collect()).issuperset(set(spark.createDataFrame(data=data, schema=[\"namespace\"]).collect())), \"No has creado todas las bases de datos solicitadas.\"\n",
    "\n",
    "except:\n",
    "    nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
    "    raise\n",
    "\n",
    "nombres_a_crear = [\"ventas_db\", \"inventario_db\", \"activos_db\"]\n",
    "\n",
    "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "688845290fb9e2c43d2b24179a428591",
     "grade": false,
     "grade_id": "ej4-enun-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16a4c387295fe299b342ae1e450c86c7",
     "grade": false,
     "grade_id": "ej4-enun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ahora que existe la bases de datos `ventas_db`, podemos cargar nuestras tablas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a46584c6bac360104b1c542f47dcb92",
     "grade": false,
     "grade_id": "ej4-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implementa una función llamada `cargar_a_ventas_db` que no reciba parámetros, cargue las tablas `spark_ordenes_df`, `spark_clientes_df` y `spark_agentes_df` a la base de datos `ventas_db` y retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener una única columna llamada `\"tableName\"`, cuyos valores sean las tablas en la base de datos `ventas_db`. Los nombres de las tablas en la base de datos deben ser los mismos de las variables en `pyspark`. La función no debe crear las tablas que ya existan en la base de datos. \n",
    "\n",
    "La función debe retornar un `DataFrame` de `pyspark`.\n",
    "\n",
    "Ejecuta tu función y guarda el resultado en una variable global llamada `spark_tablas_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5c1f1559b445cab02401285481cc494",
     "grade": false,
     "grade_id": "ej4_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1e81ffc3d21803c84e0a4b8f5c6db2",
     "grade": true,
     "grade_id": "ej4_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## AUTO-CALIFICADOR\n",
    "\n",
    "# Base variables\n",
    "import pyspark\n",
    "\n",
    "# Caso 1: no existe la función.\n",
    "try:\n",
    "    cargar_a_ventas_db\n",
    "    assert callable(cargar_a_ventas_db)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe una función llamada cargar_a_ventas_db.\",)\n",
    "\n",
    "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
    "try:\n",
    "    resultado = cargar_a_ventas_db()\n",
    "except:\n",
    "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")    \n",
    "\n",
    "# Caso 3: no retorna un DataFrame.\n",
    "assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
    "\n",
    "# Caso 4: retorna un dataframe con cantidad de columnas errada\n",
    "assert len(resultado.columns) == 1, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "\n",
    "# Caso 5: devuelve un dataframe con cantidad de filas errada\n",
    "assert resultado.count() == 3, \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes y que tu base de datos tenga las tablas estrictamente necesarias.\"\n",
    "\n",
    "# Caso 6: los dataframe no contienen las columnas debidas\n",
    "assert \"tableName\" in resultado.columns, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar las columnas.\"\n",
    "\n",
    "# Caso 7: retorna valores no acertados\n",
    "tablas = [\"spark_ordenes_df\", \"spark_clientes_df\", \"spark_agentes_df\"]\n",
    "assert resultado.withColumn(\"was_requested\", functions.col(\"tableName\") \\\n",
    "                                     .isin(tablas) \\\n",
    "                                     .cast(\"long\")) \\\n",
    "                                     .agg(functions.sum(\"was_requested\")).collect()[0][0] == len(tablas), \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegúrate de no tener filas repetidas o faltantes.\"\n",
    "\n",
    "# Caso 8: no existen las variables indicadas\n",
    "try:\n",
    "    spark_tablas_df\n",
    "    assert isinstance(spark_tablas_df, pyspark.sql.dataframe.DataFrame)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe un DataFrame llamado spark_tablas_df.\",)\n",
    "    \n",
    "# Caso 9: no crea las tablas\n",
    "data = [[i] for i in tablas]\n",
    "assert set(spark.sql(\"SHOW TABLES FROM ventas_db;\").select(\"tableName\").collect()) == set(spark.createDataFrame(data=data, schema=[\"tableName\"]).collect()), \"Las tablas creadas no son estrictamente las solicitadas.\"\n",
    "\n",
    "\n",
    "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e20599200d070c1b16e470375456ae9",
     "grade": false,
     "grade_id": "ej5-enun-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9b4545ba47a266a74c0f2dc36a406aa",
     "grade": false,
     "grade_id": "ej5-enun",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Al equipo de ventas le interesa saber cuáles clientes corresponden a cada agente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b753b5747cd1331795b84d527c44a93f",
     "grade": false,
     "grade_id": "ej5-task",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implementa una función llamada `consulta_join` que no reciba parámetros, que retorne un `DataFrame` de `pyspark`. El `DataFrame` debe tener todas las columnas de las tablas `spark_agentes_df` y `spark_clientes_df`, de la base de datos `ventas_db`, de manera que para cada agente se reporten los clientes que le corresponden o `NULL` si no le corresponde ninguno. Debes ordenar el `DataFrame` por la columna `Customer_Code`.\n",
    "\n",
    "La función debe retornar un `DataFrame` de `pyspark`.\n",
    "\n",
    "Ejecuta tu función y guarda el resultado en una variable global llamada `spark_consulta_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61cc4eaf8ab84bf13a779b041002a91b",
     "grade": false,
     "grade_id": "ej5_sol",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a97f68395af6a537ed3cb398391bafb8",
     "grade": true,
     "grade_id": "ej5_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## AUTO-CALIFICADOR\n",
    "\n",
    "# Base variables\n",
    "import pyspark\n",
    "\n",
    "# Caso 1: no existe la función.\n",
    "try:\n",
    "    consulta_join\n",
    "    assert callable(consulta_join)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe una función llamada consulta_join.\",)\n",
    "\n",
    "# Caso 2: la función es interrumpida por errores durante su ejecución.\n",
    "try:\n",
    "    resultado = consulta_join()\n",
    "except:\n",
    "    raise RuntimeError(\"Tu función produce un error al ejecutarse.\")    \n",
    "\n",
    "# Caso 3: no retorna un DataFrame.\n",
    "assert isinstance(resultado, pyspark.sql.dataframe.DataFrame), f\"Tu función debe retornar un objeto de tipo '{pyspark.sql.dataframe.DataFrame.__name__}'.\"\n",
    "\n",
    "# Caso 4: retorna un dataframe con cantidad de columnas errada\n",
    "assert len(resultado.columns) == 17, \"Tu función retorna un DataFrame con cantidad de columnas errada. Revisa que no tengas columnas duplicadas o faltantes.\"\n",
    "\n",
    "# Caso 5: devuelve un dataframe con cantidad de filas errada\n",
    "assert resultado.count() == 25, \"Tu función retorna un DataFrame con cantidad de filas errada. Revisa que no tengas filas duplicadas o faltantes.\"\n",
    "\n",
    "# Caso 6: los dataframe no contienen las columnas debidas\n",
    "columnas = ['Agent_Code', 'Agent_Name', 'Agent_Working_Area', 'Agent_Commission', 'Agent_Phone_No', 'Agent_Country', 'Customer_Code', 'Customer_Name', 'Customer_City', 'Customer_Working_Area', 'Customer_Country', 'Customer_Grade', 'Customer_Opening_Amount', 'Customer_Receive_Amount', 'Customer_Payment_Amount', 'Customer_Outstanding_Amount', 'Customer_Phone_No']\n",
    "assert resultado.columns == columnas, \"Tu función retorna un DataFrame sin una o más columnas necesarias o con nombre equivocado. Asegúrate de no renombrar ni reordenar las columnas.\"\n",
    "\n",
    "# Caso 7: retorna valores no acertados\n",
    "expected_first_row = ['A008', 'Alford', 'New York', 0.12, '044-25874365', 'United States', 'C00001', 'Micheal', 'New York', 'New York', 'USA', 2, 3000, 5000, 2000, 6000, 'CCCCCCC']\n",
    "expected_last_row = ['A011', 'Ravi Kumar', 'Bangalore', 0.15, '077-45625874', 'India', 'C00025', 'Ravindran', 'Bangalore', 'Bangalore', 'India', 2, 5000, 7000, 4000, 8000, 'AVAVAVA']\n",
    "\n",
    "assert list(resultado.head(1)[0]) == expected_first_row, \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "assert list(resultado.tail(1)[0]) == expected_last_row, \"Tu función retorna un DataFrame con valores distintos a los esperados. Asegurate de no alterar el orden de los registros únicos en la tabla.\"\n",
    "\n",
    "# Caso 8: no existen las variables indicadas\n",
    "try:\n",
    "    spark_consulta_df\n",
    "    assert isinstance(spark_consulta_df, pyspark.sql.dataframe.DataFrame)\n",
    "except:\n",
    "    raise NotImplementedError(\"No existe un DataFrame llamado spark_consulta_df.\",)\n",
    "\n",
    "print(\"Felicidades, realizaste este ejercicio correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c79f24369aa0cc741bb46674c1777fd1",
     "grade": false,
     "grade_id": "refs-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Referencias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef46fdef3658a1416714291615da4f23",
     "grade": false,
     "grade_id": "refs",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "SparkBy{Examples}. Spark with Python (PySpark) Tutorial For Beginners. Recuperado el 12 de Agosto de 2022 de: \n",
    "https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "Apache PySpark. pyspark.sql.DataFrame. Recuperado el 12 de Agosto de 2022 de: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html\n",
    "\n",
    "w3resource. SQL Table. Recuperado el 20 de Agosto de 2022 de: https://www.w3resource.com/sql/sql-table.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e40d2a9d99b7552b736f47b3289cf1f",
     "grade": false,
     "grade_id": "creds-titulo",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Créditos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60a09939fed82859b9b28289a0f8e69f",
     "grade": false,
     "grade_id": "creds",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Autor(es)**: Alejandro Mantilla Redondo, Diego Alejandro Cely\n",
    "\n",
    "**Fecha última actualización:** 14/09/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
